# Stroke-Detection


# ğŸ§  Stroke Prediction Using Machine Learning

This project explores the development of a predictive machine learning model to estimate the risk of stroke using patient health and demographic data. Given the clinical importance of early detection, the focus is on maximizing **recall**, even at the expense of some false positives.

---

## ğŸ“Œ Project Goal

To build an interpretable and balanced machine learning model that accurately identifies individuals at high risk of stroke, while handling class imbalance and providing insights through visual and statistical evaluation.

---

## ğŸ“Š Dataset Overview

- **Total observations**: 5,110
- **Target variable**: `stroke` (1 = stroke occurred, 0 = no stroke)
- **Class imbalance**: Only ~4.8% of individuals had a stroke

---

## âš–ï¸ Why SMOTE Was Used

The stroke class is significantly underrepresented (~5%). Using standard training without balancing leads to models that perform well on accuracy but **fail to detect stroke cases (low recall)**.

**SMOTE (Synthetic Minority Over-sampling Technique)**:
- Generates synthetic examples of the minority class in the training set.
- Prevents models from being biased toward the majority class.
- Increases model sensitivity to stroke cases.

ğŸ§  **Result**: After applying SMOTE, the model could learn to detect stroke patterns better, improving **recall and F1-score** for the minority class.

---

## âš™ï¸ Preprocessing Steps

- Imputed missing BMI values using median.
- Capped and log-transformed outliers in `age`, `avg_glucose_level`, `bmi`.
- Encoded categorical variables using one-hot encoding.
- Standardized numerical features for linear models.
- Created two datasets: one for linear models, one for tree-based models.

---

## ğŸ¤– Models Trained

- Logistic Regression âœ… (final model)
- Random Forest ğŸŒ² (tested for comparison)
- XGBoost âš¡ (tested for comparison)

Initially, all models performed well on accuracy due to imbalance, but performed poorly in detecting stroke cases (low recall for class 1).

---

## ğŸ¯ Threshold Tuning

At the default threshold = 0.5, the Logistic Regression model had:  
- **Precision (stroke=1):** ~0.14  
- **Recall (stroke=1):** ~0.74  
- **F1 Score:** ~0.23  
- **Accuracy:** ~0.76  

ğŸ” **Insight:** Very high recall but very low precision â†’ too many false positives.

After tuning to **threshold = 0.75**, the model showed:

| Metric              | Value  |
|---------------------|--------|
| Precision (stroke=1) | 0.22   |
| Recall (stroke=1)    | 0.58   |
| F1 Score            | 0.32   |
| Accuracy            | 0.88   |
| ROC AUC             | ~0.83  |

âœ… **Better balance** â€” improved precision while still maintaining moderate recall, leading to higher accuracy overall.

## ğŸ“ˆ Graph Interpretations

- ğŸ”· **Confusion Matrix:** Shows how many true stroke cases were correctly identified (TP), and how many were missed (FN). At threshold = 0.75, some positive cases are missed, but precision improves compared to threshold 0.5, reducing false positives.  
- ğŸŸ  **ROC Curve:** Plots True Positive Rate vs False Positive Rate for all thresholds. AUC ~0.83 â†’ the model is good at ranking positive vs negative cases.  
- ğŸŸ¢ **Precision-Recall Curve:** Visualizes the tradeoff between precision and recall. Useful when dealing with imbalanced datasets. Helps identify the ideal threshold that gives the best F1-score.

## âœ… Conclusion

Logistic Regression with SMOTE and threshold tuning to 0.75 provides a strong balance for stroke detection. Prioritizing recall ensures fewer missed high-risk patients, which is critical in healthcare. The AUC of 0.83 demonstrates strong discriminatory ability, and the threshold choice improves precision without losing too much recall.

---

## ğŸ¥ Future Healthcare Applications

1. **Clinical Screening Tool** â€” EHR-integrated model to flag high-risk patients.
2. **Remote Monitoring** â€” Could power wearables/apps for continuous stroke risk updates.
3. **Population Health** â€” Applied to registries to identify high-risk groups for intervention.
4. **Decision Support** â€” Clinicians can use model outputs alongside other indicators.
5. **Explainability Tools** â€” Add SHAP or LIME to help clinicians trust and interpret predictions.

---
